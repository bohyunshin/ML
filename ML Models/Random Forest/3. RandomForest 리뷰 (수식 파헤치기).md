## Random Forest

**ML 알고리즘 파헤치기 시리즈 !** 이번에는 Random Forest (RF)을 살펴보고자 한다. RF는 비선형 모델의 대표 주자로, CART에 기반한 ensemble 모형이다. 2001년에 Breiman에 의해서 발표되었으며 ensemble learning의 시작점이라고 해도 과언이 아니다. RF가 가지는 특징은 아래와 같이 요약할 수 있다.

* Ensemble learning: 여러 개의 CART가 모여서 RF를 이룬다. 하나의 CART는 과적합의 위험이 있기 때문에 여러 개의 CART를 적합하는 것이다. 
* Robust to Outliers: 모든 데이터에는 이상치가 있기 마련이다. 그 이상치가 기입 오류가 됐든, 실제 데이터 분포에서 동떨어졌든, 조심스럽게 다뤄야하고 함부로 삭제하는 것은 좋지 않은 해결책이다. RF는 표본을 boostraping하기 때문에 outliers에 robust하다는 특징이 있다.
* NOT OVERFITTING: 사실 가장 중요한 특징 중 하나이다. RF는 CART에 비해서 과적합의 위험이 없다. 이 논문의 정수가 바로 여기에 있는데 과적합되지 않는 다는 것을 Strong of Law of Large Numbers로 밝히기 때문이다. 이에 대한 증명 과정은 이후에 자세히 살펴보도록 하자.



본격적으로 RF를 자세하게 뜯어보도록 하자. 아래 내용은 Breiman의 논문 내용을 참조하였음을 미리 밝혀둔다.



### 1. Characherizing the accuracy of random forests

### 1.1 Random forests converge

여기서는 RF가 왜 NOT OVERFITTING인지 살펴본다. $K$ 개의 ensemble of classifiers, $h_1(\mathbf{x}), \cdots, h_K(\mathbf{x})$ 와 random vector $Y, \mathbf{X}$ 를 가정하자. margin function을 아래와 같이 정의한다.
$$
mg(\mathbf{X},Y) = \dfrac1B \sum_b I(h_b(\mathbf{X})=Y) - \underset{j \neq Y}{max} \dfrac1B \sum_b I(h_b(\mathbf{X})=j) \tag1
$$
위 margin function의 첫 번째 term은 $K$ 개의 tree가 제대로 분류한 비율을 뜻하고 두 번째 term은 오뷴류할 최대 비율을 뜻한다. 만약 $K$ 개의 나무가 완전히 정확하게 맞추면 위 값은 1이고, 완전히 틀리면 -1이다. margin function이 -1에서 0 사이면, 이 모형은 잘 못맞추고 있다는 뜻이다. margin function이 크면 클수록, 우리는 분류 결과를 신뢰할 수 있다.

margin function을 이용한 generalization error는 다음과 같다.
$$
PE^* = P_{\mathbf{X},Y} (mg(\mathbf{X,Y}) < 0) \tag2
$$
generalization error를 잘살펴보면, margin function이 0보다 작을 확률이고, 이는 곧 분류기가 제 성능을 발휘하지 못할 확률로 해석할 수 있다. 이러한 맥락에서 얘의 이름도 generalization **error** 라고 지은 것 같다.
여기서 주목해야할 점은 (2)의 generalization error가 sample version이라는 것이다. 즉, margin function이 애초에 모집단이 아니라 sample로 부터 계산된 추정량이다. 그렇다면, 이 추정량이 바라보는 모수가 있을 것이고 Breiman은 (2)가 아래의 population generalization error로 a.s. 수렴함을 수학적으로 보였다.
$$
P_{\mathbf{X},Y}(P_\theta (h(\mathbf{X},\theta)=Y) - \underset{j \neq Y}{max} P_\theta (h(\mathbf{X},\theta)=j) < 0  ) \tag3
$$
*proof)*

$h_b(\mathbf{X}) = h(\mathbf{X}, \theta_b)$ 라고 하자. $\theta_b$ 는 각각의 나무와 관련된 parameters라 하자. 다음을 보이는 것으로, (2)가 (3)으로 a.s. 수렴한다는 것을 충분히 보일 수 있다.
$$
\dfrac1B \sum_b I(h(\theta_b, \mathbf{x}) = j) \rightarrow P_\theta (h(\theta, \mathbf{x})=j)
$$

* 고정된 훈련 데이터와, 모수 $\theta$ 하에서 $h(\theta, \mathbf{x})=j$ 를 만족하는 모든 $\mathbf{x}$ 는 hyper rectangles의 union이이고 이를 $S_1, \cdots, S_k$ 라고 하자. 
* 만약에 $\left[ \mathbf{x}: h(\theta, \mathbf{x})=j \right]=S_k$ 라면 $\psi(\theta) = k$ 라고 정의하자.
*  $N_k$ 를 처음 $B$ 번의 boostraping samples에서 $\psi(\theta_b)=k$ 인 횟수라고 하자.

그러면,
$$
\dfrac1B \sum_b I(h(\theta_b, \mathbf{x})=j) = \dfrac1B \sum_k N_k I(\mathbf{x} \in S_k)
$$
위 식의 의미를 나름대로 고민해보았는데, 우선 $S_k$ 부터 음미해야할 것 같다. $S_k$ 는 $h(\theta, \mathbf{x})=j$ 를 만족하는 모든 $\mathbf{x}$ 는 hyper recatangles인 $S_k$의 합집합이다. 즉, $h(\theta, \mathbf{x})=j$ 을 만족하는 모든 $\mathbf{x}$ 는 $S_1, \cdots, S_K$ 에 conditional 하게 속한다고 볼 수 있고, $h(\theta, \mathbf{x})=j$  에 대한 indicator function을 $S_k$ 에 대한 indicator function으로 바꿀 수 있다. 그런데 summation의 첨자까지 바뀐 것을 볼 수 있다. 따라서, $h(\theta_b, \mathbf{x})=j$ 에서 모든 $b$ 에 대해 고려하기 위해서, $I(\mathbf{x} \in S_k) $ 에도 $N_k$ 를 곱해준다.

다음으로 아래의 관계식이 나온다.
$$
N_k =  \sum_b I(\psi(\theta_b)=k)
$$
$N_k$ 의 정의를 다시 짚고 가자. $N_k$ 는 처음 $B$ 번의 bootstraping samples에서 $\psi(\theta_b)=k$ 인 횟수이다. 따라서 위 식은 정의를 그대로 적은 것이다. 여기서 LLN을 사용하기 위해,
$$
\dfrac{N_k}{B} = \dfrac1B \sum_b I(\psi(\theta_b)=k) \rightarrow P_\theta(\psi(\theta)=k)
$$
따라서 
$$
\dfrac1B \sum_b I(h(\theta_b, \mathbf{x})=j) = \dfrac1B \sum_k N_k I(\mathbf{x} \in S_k)
		\rightarrow \sum_k P_\theta(\psi(\theta)=k)I(\mathbf{x}\in S_k) = P_\theta(h(\theta,\mathbf{x})=j)
$$
증명 끝!



증명의 결과를 좀 더 음미해보자. $\dfrac1B \sum_b I(h(\theta_b, \mathbf{x}) = j) \rightarrow P_\theta (h(\theta, \mathbf{x})=j)$ 가 의미하는 바는, 나무의 개수가 많아지면 true probability로 수렴한다는 뜻이다. 즉, CART를 많이 사용한다고 해서 overfitting될 위험이 없다는 것이다. 다만, computation의 한계가 있어서 적절하게 나무의 개수를 정해야할 것이다.



**아직 밝히지 못한 점**

* $\dfrac1B \sum_b I(h(\theta_b, \mathbf{x}) = j) \rightarrow P_\theta (h(\theta, \mathbf{x})=j)$ 을 밝힌다고 해서, sample GE -> population GE가 되는 맥락을 모르겠다.



### Strength and correlation

RF에서, generalization error의 upper bound을 두 개의 모수로 표현할 수 있다: 각각의 분류기가 얼마나 정확한지, 그들간의 dependence가 바로 그것이다. 이 둘을 음미하는 것이 RF의 또 다른 핵심이라고 해도 과언이 아니다. 이제부터 이를 살펴보자.

우리가 도달해야하는 결론은 아래와 같다.
$$
PE^* \leq \bar{\rho} (1-s^2)/s^2 \tag4
$$
*proof)*

RF의 population margin function을 아래와 같이 정의하자.
$$
mr(\mathbf{X},Y) = P_\theta (h(\mathbf{X}, \theta)=Y) - \underset{j\neq Y}{max} P_\theta(h(\mathbf{X},\theta)=j) \tag5
$$
분류기 $h(\mathbf{x}, \theta)$ 의 strength는
$$
s = E_{\mathbf{X},Y}mr(\mathbf{X},Y)
$$
$s \geq0$ 이라는 가정 하에, chebychev's 부등식을 이용하면
$$
PE^* \leq var(mr) / s^2
$$
(5)의 population margin function을 다시 정의해보자.
$$
\hat{j}(\mathbf{X},Y) = \underset{j \neq Y}{argmax} P_\theta (h(\mathbf{X}, \theta)=j)
$$
라고 하면, 
$$
\begin{split}
mr(\mathbf{X},Y) &= P_\theta (h(\mathbf{X}, \theta)=Y) - P_\theta(h(\mathbf{X},\theta)=\hat{j}(\mathbf{X},Y)) \\
&= E_\theta [ I(h(\mathbf{X},\theta)=Y) - I(h(\mathbf{X}, \theta)=\hat{j}(\mathbf{X},Y)) ]
\end{split}
$$
raw margin function을 정의하자.
$$
rmg(\theta, \mathbf{X}, Y)=I(h(\mathbf{X},\theta)=Y) - I(h(\mathbf{X}, \theta)=\hat{j}(\mathbf{X},Y))
$$
따라서 $mr(\mathbf{X},Y) = E_\theta[rmg(\theta,\mathbf{X},Y)]$ 이다.

서로 독립인 $\theta, \theta'$ 에 대해서, $mr(\mathbf{X},Y)^2 = E_{\theta, \theta'}rmg(\theta,\mathbf{X},Y)rmg(\theta',\mathbf{X},Y) $ 이기 때문에
$$
\begin{split}
var(mr) &= E[mr^2] - E[mr]^2 \\
				&= E \left[ E_{\theta, \theta'}rmg(\theta,\mathbf{X},Y)rmg(\theta',\mathbf{X},Y) \right] - s^2 \\
				&= E_{\theta,\theta'}[cov \; rmg(\theta,\mathbf{X},Y)rmg(\theta',\mathbf{X},Y) ] - s^2 \\
				&\leq E_{\theta, \theta'} \rho(\theta,\theta')sd(\theta)sd(\theta') \\
				&= \bar\rho (E_{\theta}sd(\theta))^2 \\
				&\leq \bar\rho E_{\theta}var(\theta)
\end{split} \tag6
$$
여기서 $\bar\rho$ 는 correlation의 mean value이다. 그런데
$$
E_\theta var(\theta) \leq E_\theta (E_{\mathbf{X},Y}rmg(\theta,\mathbf{X},Y))^2 -s^2 \leq 1-s^2 \tag7
$$
이므로 (4), (6), (7)을 종합하면 증명이 완성된다.
$$
PE^* \leq \bar\rho (1-s^2)/s^2 \tag8
$$
(8)의 $PE^*$ 에 대한 upper bound는 $s, \bar\rho$ 로 구성되어 있다. $s$ 는 forest의 개별 분류기, 즉 각 나무들의 strength이고 $\bar\rho$ 는 raw margin function들의 correlation에 대한 mean value이다.



### 2. Using random features

RF의 정확도를 높이기 위해서 generalization error를 줄여야하고, (8)의 upper bound을 통해서 $s$ 은 고정시킨채로 $\bar\rho$ 를 최소로 만드는 전략을 사용해야함을 알 수 있다. 이를 위해 Breiman은 randomly selected inputs 또는 combinations of inputs을 각 나무를 키우기 위해 각 node에 사용한다. 이러한 절차는 아래의 이점이 있다고 한다.

* Adaboost만큼 정확도가 높아진다.
* outliers와 noise에 robust하다.
* bagging, boosting보다 빠르다.
* 유용한 error의 추정치, strength, correlation, variable importance을 제공한다.
* 간단하고 쉽게 병렬화할 수 있다.

### 2.1 Using out-of-bag estimates to monitor error, strength and correlation

Breiman은 bagging과 함께 random feature selection을 사용하였다. bagging을 사용하는 이유는 다음과 같다.

* bagging과 random feature selection을 함께 사용하면 정확도가 향상된다.
* generalization error, strength, correlation을 추정할 수 있다.

사실 두 번째 이유가 강력하다. bootstrap된 훈련 데이터를 $T_k$ 라 하고 이를 이용하여 분류기 $h(\mathbf{x}, T_k)$ 를 작성한다. 데이터 세트에는 bootstrap되지 않은 데이터도 있을 것이다. 이 데이터들에 대해서 $h(\mathbf{x}, T_k)$ 를 적용하고, 여러 개의 bagging된 분류기가 있을 것이므로 예측값들을 종합한다. 이를 bagged predictor라 하고 bagged predictor가 맞추지 못하는 비율을 out-of-bag estimate for the generalization error라고 한다.

이뿐만 아니라 strength, correlation도 out-of-bag 방법을 이용하여 추정할 수 있다고 한다. (Appendix 2 생략...)



### 3. Variable Importance

RF의 가장 큰 장점 중 하나는 변수 중요도를 제공한다는 것이다. RF는 비선형 모형으로, 선형 모형보다는 해석력이 떨어질 수밖에 없다. 따라서 혹자는 black box로 분류하기도 하는데, RF는 변수 중요도도 제공할 뿐만 아니라 tree interpreter, 더 크게는 LIME에 의해서 해석될 수도 있어서 개인적으로는 black box라고 분류하는 것은 조금 무리가 있다고 생각한다. 아무튼, RF에서 variable importance을 어떻게 계산하는지 알아보자.

